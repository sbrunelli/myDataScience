{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping football match results from the web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code to scrape all match statistics from the website http://www.legaseriea.it/it.\n",
    "\n",
    "The goal of this scraper is to extract the data without performing any transformation on them. \n",
    "\n",
    "I am a strong fan of software modularity and I believe, and my experience confirms it, a software system that works is one made of small components, each one of them with a very well defined set of responsibilities and it must be good at that and that alone.\n",
    "\n",
    "This component has **2 main such responsibilities**:\n",
    "\n",
    "1. Extract all the available statistics from each match report of each season available from the web site archive.\n",
    "2. Dump them to a .csv file for further processing from other modules.\n",
    "\n",
    "Because the scraping operations typically require a significant amount of time, and after seing the program fail a couple of times I decided to dump each single season right after its extraction was complete and then procede with the next one.\n",
    "\n",
    "Therefore in the data directory you'll find one file for each season.\n",
    "\n",
    "Before dumping the file to disk, I transformed it to a python pandas data frame. No attention has been given to the order of the columns at this point. Later on, once I'll read in all the .csv into a single dataframe, I'll take care of that, and also give the columns english short names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all I load all the necessary libraries at the beginning of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions in this section will be used to transform the data scraped from the website into a data frame and then dump them to the filesystem in the form of a csv file.\n",
    "\n",
    "The **build_column_names** function has the following responsibility: each season potentially has a different set of statistics that have been collected, therefore the first thing to do is extract these statistics names as they will be used to name the columns of the data frame and will become the headers of the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "# This function builds the columns names for the tabular representation of the data\n",
    "# extracted by the scraper\n",
    "############################################################################################\n",
    "def build_column_names(m):\n",
    "    # Fixed fileds, present in any given season\n",
    "    cnames = [\"Stagione\", \"Giornata\", \"Squadra\", \"Avversaria\", \"Campo\", \"Gol segnati\", \"Gol subiti\"]\n",
    "    \n",
    "    # Each of the following season can have more or less fields, so we append them\n",
    "    # dynamically scanning the extracted statistics starting from the part\n",
    "    # where we have stored the tuples (statname, hvalue, avalue).\n",
    "    # \n",
    "    # For any given statname, we also include its passive version, the value \n",
    "    # of the same statistic for the opponent of a given team\n",
    "    if len(m) > 6:\n",
    "        for statname, dummy1, dummy2 in m[6:]:\n",
    "            cnames.append(statname)\n",
    "            cnames.append(statname + \" avversario\")\n",
    "    return(cnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I extract the data from the HTML code I create a list of lists, where each single list element of the overall list hs a structure of this kind:\n",
    "\n",
    "(season,   game_number,   home_team,   away_team,   home_goals,   away_goals,   (statistics_name, home_value, away_value))\n",
    "\n",
    "This is because, while the first 6 fields are always present, statistics are not. As seasons progresses more statistics have been collected, so the first seasons have only a limited set of stats, while the most recent ones have a much richer library. In order to allow flexibility to the data structure, I used this kind of EAV (entity attribute value) tuple, of which I can have as many as I want without bother about their position in the list.\n",
    "\n",
    "Later I'll take care of unpacking it when building the data frame.\n",
    "\n",
    "By the way, this is the reason why columns will appear in that scrambled order in the .csv, but like I said, column order is not the responsibility of the scraper, so I'll let it be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# This function scans the extracted statistics and it build 2 lists:\n",
    "#\n",
    "#  1. all the statistics related to the home team, both active and passive\n",
    "#  2. all the statistics related to the away team, both active and passive\n",
    "###############################################################################\n",
    "def build_statistics_lists(game):\n",
    "    hlst = []\n",
    "    alst = []\n",
    "    # season\n",
    "    hlst.append(game[0])\n",
    "    alst.append(game[0])\n",
    "    # game day\n",
    "    hlst.append(game[1])\n",
    "    alst.append(game[1])\n",
    "    # home team\n",
    "    hlst.append(game[2])\n",
    "    # away team\n",
    "    alst.append(game[3])\n",
    "    # home opponent\n",
    "    hlst.append(game[3])\n",
    "    # away opponent\n",
    "    alst.append(game[2])\n",
    "    # Venue\n",
    "    hlst.append(\"home\")\n",
    "    alst.append(\"away\")\n",
    "    # home goals scored\n",
    "    hlst.append(game[4])\n",
    "    # home goals allowed\n",
    "    hlst.append(game[5])    \n",
    "    # away goals scored\n",
    "    alst.append(game[5])\n",
    "    # away goals allowed\n",
    "    alst.append(game[4])\n",
    "    # all game statistics\n",
    "    if len(game) > 6:\n",
    "        for statname, hvalue, avalue in game[6:]:\n",
    "            hlst.append(hvalue)\n",
    "            hlst.append(avalue)\n",
    "            alst.append(avalue)\n",
    "            alst.append(hvalue)\n",
    "    return (hlst, alst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I read one element of the overall list, which also happens to be a list of its own and create 2 data frames:\n",
    "\n",
    "1. _dfhome_ will contain all the stats for the home team\n",
    "2. _dfaway_ will contain all the stats for the away team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################\n",
    "# This function reads 2 lists of lists:\n",
    "#\n",
    "#  1. all statistics for home teams\n",
    "#  2. all statistics for away teams\n",
    "#\n",
    "# and it buils a data frame representation from them\n",
    "################################################################\n",
    "def build_data_frame(hteamstats, ateamstats, cols):\n",
    "    dfhome = pd.DataFrame([hteamstats], columns = cols)\n",
    "    dfaway = pd.DataFrame([ateamstats], columns = cols)\n",
    "    df = dfhome.append(dfaway)\n",
    "    return (df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function that take a data frame, and dumps it to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# This function stores the extracted data to a csv file\n",
    "##########################################################\n",
    "def dump_df_to_csv(df, season):\n",
    "    save_path = \"./data/\"\n",
    "    fname = season.replace(\"/\",\"_\") + \".csv\"\n",
    "    completeName = os.path.join(save_path, fname)\n",
    "    fhand = open(completeName, \"w\")\n",
    "    df.to_csv(fhand, index=False)\n",
    "    fhand.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function loops through all the statistics extracted for a single season and take care of the entire process of converting it to a data frame and than saving it to a file.\n",
    "\n",
    "It is like the _main_ of the dumping process, with the only responsibility of looping through the list and calling the right functions in the right order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "# This function takes care of the entire process of converting the extracted statistics\n",
    "# into a data frame and dumping it to a .csv file\n",
    "##########################################################################################\n",
    "def store_statistics(seas, stats):\n",
    "    csv = pd.DataFrame(columns = [\"dummy\"])\n",
    "    for match in stats:\n",
    "        cnames = build_column_names(match)\n",
    "        hval, aval = build_statistics_lists(match)\n",
    "        df = build_data_frame(hval, aval, cnames)\n",
    "        csv = pd.concat([csv, df], axis = 0, ignore_index=True)\n",
    "    del csv[\"dummy\"]\n",
    "    dump_df_to_csv(csv, seas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions in this section will be used to scrape the actual data from the website:\n",
    "\n",
    "Scraping will be organized at different levels of detail:\n",
    "\n",
    ">1. Scraping a single match report: this is actuall the function that implements the scraping logic.\n",
    ">2. Scraping all games of a particular match day.\n",
    ">3. Scraping all match days of a particular season.\n",
    ">4. Scraping all seasons contained in the archive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the core function, the one which scrapes the actual match reports.\n",
    "\n",
    "In doing so it must take care of some minor peculiarities of some seasons, as far as the structure of the HTML code.\n",
    "\n",
    "It builds and returns a list with the following structure:\n",
    "\n",
    "* **opening fields**: season, match number, home team, away team, home goals, away goals\n",
    "* **actual match statistics**: this will be organized as tuples like (statname, home value, away value), for example (\"shots on goal\", 7, 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "# This function scrapes one single match report.\n",
    "# It takes the url of the match report in input.\n",
    "# It return all the statistics as a list where with the following structure:\n",
    "#\n",
    "# - season\n",
    "# - game day\n",
    "# - home team name\n",
    "# - away team name\n",
    "# - home team goals scored\n",
    "# - away team goals scored\n",
    "# - as many tuples of the form\n",
    "#        (statname, home value, away value)\n",
    "#   as there are statistics in the report\n",
    "##################################################################################\n",
    "def scrape_match_report(url):\n",
    "    try:\n",
    "        print (\"MATCH REPORT: Scraping url...\", url)\n",
    "\n",
    "        # Open the url, parse it and close the connection immediately\n",
    "        page = urllib.request.urlopen(url)\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "        page.close()\n",
    "\n",
    "        # Initialize return list\n",
    "        lst = []\n",
    "\n",
    "        # Extract season and matchday information form the url\n",
    "        season = re.findall(\".*match-report/([0-9]+-[0-9]+)/.*\", url)\n",
    "        matchday = re.findall(\".*UNI/([0-9]+)/.*\", url)\n",
    "        lst.append(season[0])\n",
    "        lst.append(matchday[0])\n",
    "\n",
    "        # Extract team names and goals scored\n",
    "        report_risultato = soup.find(class_ = \"report-risultato\")\n",
    "        hteam = report_risultato.find(class_ = \"report-squadra squadra-a\").span.text\n",
    "        ateam = report_risultato.find(class_ = \"report-squadra squadra-b\").span.text\n",
    "        hgoal = report_risultato.find(class_ = \"squadra-risultato squadra-a\").text\n",
    "        agoal = report_risultato.find(class_ = \"squadra-risultato squadra-b\").text\n",
    "        ###print(hteam, ateam, hgoal, agoal)\n",
    "        lst.extend([hteam, ateam, hgoal, agoal])\n",
    "\n",
    "        # Extract all match statistics\n",
    "        statistiche_comparate = soup.find(id = \"statistiche-comparate\")\n",
    "        statnames_rs = statistiche_comparate.find_all(class_ = \"valoretitolo\")\n",
    "        hvalues_rs = statistiche_comparate.find_all(class_ = \"valoresx\")\n",
    "        avalues_rs = statistiche_comparate.find_all(class_ = \"valoredx\")\n",
    "        for i in range(len(statnames_rs)):\n",
    "            if (season[0] >= \"2014-15\"):\n",
    "                if (i==0):                     \n",
    "                    lst.append((statnames_rs[i].text, hvalues_rs[i].text, avalues_rs[i].text))\n",
    "                elif (i==1):\n",
    "                    continue\n",
    "                else:\n",
    "                    lst.append((statnames_rs[i].text, hvalues_rs[i-1].text, avalues_rs[i-1].text))                \n",
    "            else:\n",
    "                lst.append((statnames_rs[i].text, hvalues_rs[i].text, avalues_rs[i].text))\n",
    "\n",
    "        return lst\n",
    "    except:\n",
    "        # On exception we dump the function's execution context\n",
    "        fhand = open(\"scrape_match_report.err\", \"w\")\n",
    "        fhand.write(\"{}\\n\\n\".format(url))\n",
    "        fhand.write(soup.decode(\"utf-8\"))\n",
    "        fhand.close()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function loops through a given match day of the current season and extracts all the links to all the match reports and than calls the scrape_match_report function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "# This function scrapes one single match day.\n",
    "# As input it takes:\n",
    "#\n",
    "#  1. a list of stats\n",
    "#  2. the url of the match day page\n",
    "#\n",
    "# It extract all links to match reports within the page and processes them\n",
    "# one at a time.\n",
    "##################################################################################\n",
    "def scrape_match_day(stats, url):\n",
    "    try:\n",
    "        print(\"\\n\\nGAMEDAY: Scraping url...\", url)\n",
    "        # Base domain variable\n",
    "        domain = \"http://www.legaseriea.it\"    \n",
    "\n",
    "        # Open the url, parse it and close the connection immediately\n",
    "        page = urllib.request.urlopen(url)\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "        page.close()\n",
    "        \n",
    "        # Extract season and matchday information form the url\n",
    "        season = re.findall(\".*archivio/([0-9]+-[0-9]+)/UNICO.*\", url)\n",
    "        \n",
    "        # Find all links to match-reports within current matchday\n",
    "        # Since 2015-16 there's an additional match-program link that we\n",
    "        # have to skip        \n",
    "        matchreports = soup.find_all(class_ = \"link-matchreport\")\n",
    "        \n",
    "        for matchreport in matchreports:\n",
    "            if (season[0] == \"2015-16\"):\n",
    "                stats.append(scrape_match_report(domain + matchreport.find_all(\"a\")[1]['href']))\n",
    "            else:\n",
    "                stats.append(scrape_match_report(domain + matchreport.a['href']))\n",
    "\n",
    "    except:\n",
    "        # On exception we dump the function's execution context\n",
    "        fhand = open(\"scrape_match_day.err\", \"w\")\n",
    "        fhand.write(\"{}\\n\\n\".format(url))\n",
    "        fhand.write(soup.decode(\"utf-8\"))\n",
    "        fhand.close()         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function loops through a given season and extracts all the links to all the match days and than calls the scrape_match_day function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "# This function scrapes one single season.\n",
    "# As input it takes:\n",
    "#\n",
    "#  1. a list of stats\n",
    "#  2. the url of the season page\n",
    "#\n",
    "# It extract all links to all match days within the page and processes them\n",
    "# one at a time.\n",
    "##################################################################################\n",
    "def scrape_season(url):\n",
    "    try:\n",
    "        stats = []\n",
    "        print(\"\\n\\n\\n\\n======\\nSEASON\\n======\")\n",
    "        # Base domain variable\n",
    "        domain = \"http://www.legaseriea.it\"    \n",
    "\n",
    "        # Open the url, parse it and close the connection immediately\n",
    "        page = urllib.request.urlopen(url)\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "        page.close()\n",
    "\n",
    "        # Find all links to all match days of current season\n",
    "        matchdays_first_half_season = soup.find_all(class_ = \"box_Ngiornata_andata\")\n",
    "        for matchday in matchdays_first_half_season:\n",
    "            stats.append(scrape_match_day(stats, domain + matchday.a['href']))\n",
    "            stats.pop()\n",
    "\n",
    "        matchdays_second_half_season = soup.find_all(class_ = \"box_Ngiornata_ritorno\")\n",
    "        for matchday in matchdays_second_half_season:\n",
    "            stats.append(scrape_match_day(stats, domain + matchday.a['href']))      \n",
    "            stats.pop()\n",
    "\n",
    "        return stats\n",
    "    except:\n",
    "        # On exception we dump the function's execution context\n",
    "        fhand = open(\"scrape_season.err\", \"w\")\n",
    "        fhand.write(\"{}\\n\\n\".format(url))\n",
    "        fhand.write(soup.decode(\"utf-8\"))\n",
    "        fhand.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function loops through the entire archive and does the following:\n",
    "\n",
    "1. Extracts all the season contained in the archive.\n",
    "2. For each one of these builds a link to the corresponding page.\n",
    "3. Passes this link to the scrape_season function\n",
    "4. Call the store_statistics function at the end of the season scraping process to dump the data to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "# This function scrapes the entire archive.\n",
    "# As input it takes a list of stats.\n",
    "# It builds all links to all seasons within the page and processes them\n",
    "# one at a time.\n",
    "##################################################################################\n",
    "def scrape_archive():\n",
    "    try:\n",
    "        # Open the url, parse it and close the connection immediately\n",
    "        page = urllib.request.urlopen(\"http://www.legaseriea.it/it/serie-a-tim/archivio\")\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "        page.close()    \n",
    "\n",
    "        # Extract list of all available seasons and sort it ascending\n",
    "        archivio_stagione_id = soup.find(id = \"archivio_stagione_id\")\n",
    "        options = archivio_stagione_id.find_all(\"option\")\n",
    "        seasons = []\n",
    "        for option in options:\n",
    "            seasons.append(option['value'])        \n",
    "        seasons.sort()\n",
    "\n",
    "        # For each season call the scrape_season method\n",
    "        for season in seasons:   \n",
    "            url = \"http://www.legaseriea.it/it/serie-a-tim/archivio/\" + season            \n",
    "            stats = scrape_season(url)\n",
    "###            return (stats)\n",
    "            store_statistics(season, stats)\n",
    "\n",
    "    except:\n",
    "        # On exception we dump the function's execution context\n",
    "        fhand = open(\"scrape_archive.err\", \"w\")\n",
    "        fhand.write(\"{}\\n\\n\".format(url))\n",
    "        fhand.write(soup.decode(\"utf-8\"))\n",
    "        fhand.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once everything is set comes the fun, simply run the scrape_archive function, relax, sit back and enjoy the scraping... :-)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "======\n",
      "SEASON\n",
      "======\n",
      "\n",
      "\n",
      "GAMEDAY: Scraping url... http://www.legaseriea.it/it/serie-a-tim/archivio/1986-87/UNICO/UNI/1\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/1/AVEFIO\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/1/BRENAP\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/1/EMPINT\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/1/MILASC\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/1/ROMCOM\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/1/SAMATA\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/1/TORVER\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/1/UDIJUV\n",
      "\n",
      "\n",
      "GAMEDAY: Scraping url... http://www.legaseriea.it/it/serie-a-tim/archivio/1986-87/UNICO/UNI/2\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/2/ASCEMP\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/2/ATAROM\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/2/COMTOR\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/2/FIOSAM\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/2/VERMIL\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/2/INTBRE\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/2/JUVAVE\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/2/NAPUDI\n",
      "\n",
      "\n",
      "GAMEDAY: Scraping url... http://www.legaseriea.it/it/serie-a-tim/archivio/1986-87/UNICO/UNI/3\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/3/AVENAP\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/3/BREFIO\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/3/EMPJUV\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/3/MILATA\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/3/ROMVER\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/3/SAMCOM\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/3/TORASC\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/3/UDIINT\n",
      "\n",
      "\n",
      "GAMEDAY: Scraping url... http://www.legaseriea.it/it/serie-a-tim/archivio/1986-87/UNICO/UNI/4\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/4/ASCAVE\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/4/ATAEMP\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/4/COMBRE\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/4/FIOUDI\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/4/VERSAM\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/4/INTROM\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/4/JUVMIL\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/4/NAPTOR\n",
      "\n",
      "\n",
      "GAMEDAY: Scraping url... http://www.legaseriea.it/it/serie-a-tim/archivio/1986-87/UNICO/UNI/5\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/5/ATAASC\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/5/AVECOM\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/5/FIOJUV\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/5/MILINT\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/5/ROMBRE\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/5/SAMNAP\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/5/TOREMP\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/5/UDIVER\n",
      "\n",
      "\n",
      "GAMEDAY: Scraping url... http://www.legaseriea.it/it/serie-a-tim/archivio/1986-87/UNICO/UNI/6\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/6/ASCJUV\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/6/BREUDI\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/6/COMFIO\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/6/EMPMIL\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/6/VERAVE\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/6/INTSAM\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/6/NAPATA\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/6/TORROM\n",
      "\n",
      "\n",
      "GAMEDAY: Scraping url... http://www.legaseriea.it/it/serie-a-tim/archivio/1986-87/UNICO/UNI/7\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/7/ASCCOM\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/7/ATATOR\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/7/AVEUDI\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/7/FIOVER\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/7/JUVINT\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/7/MILBRE\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/7/ROMNAP\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/7/SAMEMP\n",
      "\n",
      "\n",
      "GAMEDAY: Scraping url... http://www.legaseriea.it/it/serie-a-tim/archivio/1986-87/UNICO/UNI/8\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/8/BRESAM\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/8/COMJUV\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/8/EMPROM\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/8/VERATA\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/8/MILFIO\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/8/NAPINT\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/8/TORAVE\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/8/UDIASC\n",
      "\n",
      "\n",
      "GAMEDAY: Scraping url... http://www.legaseriea.it/it/serie-a-tim/archivio/1986-87/UNICO/UNI/9\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/9/ASCFIO\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/9/ATACOM\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/9/AVEBRE\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/9/EMPVER\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/9/INTTOR\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/9/JUVNAP\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/9/ROMUDI\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/9/SAMMIL\n",
      "\n",
      "\n",
      "GAMEDAY: Scraping url... http://www.legaseriea.it/it/serie-a-tim/archivio/1986-87/UNICO/UNI/10\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/10/BRETOR\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/10/COMINT\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/10/FIOROM\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/10/VERASC\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/10/JUVATA\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/10/MILAVE\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/10/NAPEMP\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/10/UDISAM\n",
      "\n",
      "\n",
      "GAMEDAY: Scraping url... http://www.legaseriea.it/it/serie-a-tim/archivio/1986-87/UNICO/UNI/11\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/11/ATABRE\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/11/COMUDI\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/11/EMPFIO\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/11/INTAVE\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/11/NAPVER\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/11/ROMJUV\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/11/SAMASC\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/11/TORMIL\n",
      "\n",
      "\n",
      "GAMEDAY: Scraping url... http://www.legaseriea.it/it/serie-a-tim/archivio/1986-87/UNICO/UNI/12\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/12/ASCROM\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/12/AVESAM\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/12/BREEMP\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/12/FIOINT\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/12/VERCOM\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/12/JUVTOR\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/12/MILNAP\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/12/UDIATA\n",
      "\n",
      "\n",
      "GAMEDAY: Scraping url... http://www.legaseriea.it/it/serie-a-tim/archivio/1986-87/UNICO/UNI/13\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/13/ATAAVE\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/13/BREVER\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/13/EMPUDI\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/13/INTASC\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/13/NAPCOM\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/13/ROMMIL\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/13/SAMJUV\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/13/TORFIO\n",
      "\n",
      "\n",
      "GAMEDAY: Scraping url... http://www.legaseriea.it/it/serie-a-tim/archivio/1986-87/UNICO/UNI/14\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/14/ASCBRE\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/14/AVEEMP\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/14/COMMIL\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/14/FIONAP\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/14/INTATA\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/14/JUVVER\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/14/SAMROM\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/14/UDITOR\n",
      "\n",
      "\n",
      "GAMEDAY: Scraping url... http://www.legaseriea.it/it/serie-a-tim/archivio/1986-87/UNICO/UNI/15\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/15/BREJUV\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/15/EMPCOM\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/15/VERINT\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/15/MILUDI\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/15/NAPASC\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/15/ROMAVE\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/15/TORSAM\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/15/ATAFIO\n",
      "\n",
      "\n",
      "GAMEDAY: Scraping url... http://www.legaseriea.it/it/serie-a-tim/archivio/1986-87/UNICO/UNI/16\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/16/ASCMIL\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/16/ATASAM\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/16/COMROM\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/16/FIOAVE\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/16/VERTOR\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/16/INTEMP\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/16/JUVUDI\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/16/NAPBRE\n",
      "\n",
      "\n",
      "GAMEDAY: Scraping url... http://www.legaseriea.it/it/serie-a-tim/archivio/1986-87/UNICO/UNI/17\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/17/AVEJUV\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/17/BREINT\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/17/EMPASC\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/17/MILVER\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/17/ROMATA\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/17/SAMFIO\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/17/TORCOM\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/17/UDINAP\n",
      "\n",
      "\n",
      "GAMEDAY: Scraping url... http://www.legaseriea.it/it/serie-a-tim/archivio/1986-87/UNICO/UNI/18\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/18/ASCTOR\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/18/ATAMIL\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/18/COMSAM\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/18/FIOBRE\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/18/VERROM\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/18/INTUDI\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/18/JUVEMP\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/18/NAPAVE\n",
      "\n",
      "\n",
      "GAMEDAY: Scraping url... http://www.legaseriea.it/it/serie-a-tim/archivio/1986-87/UNICO/UNI/19\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/19/AVEASC\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/19/BRECOM\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/19/EMPATA\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/19/MILJUV\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/19/ROMINT\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/19/SAMVER\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/19/TORNAP\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/19/UDIFIO\n",
      "\n",
      "\n",
      "GAMEDAY: Scraping url... http://www.legaseriea.it/it/serie-a-tim/archivio/1986-87/UNICO/UNI/20\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/20/ASCATA\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/20/BREROM\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/20/COMAVE\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/20/EMPTOR\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/20/VERUDI\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/20/INTMIL\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/20/JUVFIO\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/20/NAPSAM\n",
      "\n",
      "\n",
      "GAMEDAY: Scraping url... http://www.legaseriea.it/it/serie-a-tim/archivio/1986-87/UNICO/UNI/21\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/21/ATANAP\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/21/AVEVER\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/21/FIOCOM\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/21/JUVASC\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/21/MILEMP\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/21/ROMTOR\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/21/SAMINT\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/21/UDIBRE\n",
      "\n",
      "\n",
      "GAMEDAY: Scraping url... http://www.legaseriea.it/it/serie-a-tim/archivio/1986-87/UNICO/UNI/22\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/22/BREMIL\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/22/COMASC\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/22/EMPSAM\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/22/VERFIO\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/22/INTJUV\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/22/NAPROM\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/22/TORATA\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/22/UDIAVE\n",
      "\n",
      "\n",
      "GAMEDAY: Scraping url... http://www.legaseriea.it/it/serie-a-tim/archivio/1986-87/UNICO/UNI/23\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/23/ASCUDI\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/23/ATAVER\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/23/AVETOR\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/23/FIOMIL\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/23/INTNAP\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/23/JUVCOM\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/23/ROMEMP\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/23/SAMBRE\n",
      "\n",
      "\n",
      "GAMEDAY: Scraping url... http://www.legaseriea.it/it/serie-a-tim/archivio/1986-87/UNICO/UNI/24\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/24/BREAVE\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/24/COMATA\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/24/FIOASC\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/24/VEREMP\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/24/MILSAM\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/24/NAPJUV\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/24/TORINT\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/24/UDIROM\n",
      "\n",
      "\n",
      "GAMEDAY: Scraping url... http://www.legaseriea.it/it/serie-a-tim/archivio/1986-87/UNICO/UNI/25\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/25/ASCVER\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/25/ATAJUV\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/25/AVEMIL\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/25/EMPNAP\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/25/INTCOM\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/25/ROMFIO\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/25/SAMUDI\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/25/TORBRE\n",
      "\n",
      "\n",
      "GAMEDAY: Scraping url... http://www.legaseriea.it/it/serie-a-tim/archivio/1986-87/UNICO/UNI/26\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/26/ASCSAM\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/26/AVEINT\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/26/BREATA\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/26/FIOEMP\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/26/VERNAP\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/26/JUVROM\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/26/MILTOR\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/26/UDICOM\n",
      "\n",
      "\n",
      "GAMEDAY: Scraping url... http://www.legaseriea.it/it/serie-a-tim/archivio/1986-87/UNICO/UNI/27\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/27/ATAUDI\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/27/COMVER\n",
      "MATCH REPORT: Scraping url... http://www.legaseriea.it/it/serie-a-tim/match-report/1986-87/UNICO/UNI/27/EMPBRE\n"
     ]
    }
   ],
   "source": [
    "scrape_archive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
